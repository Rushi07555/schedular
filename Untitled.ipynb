{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedular to download file from ftp server and save data to mongdb database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client as http \n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import schedule # use to make schedule of getting data \n",
    "import time\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pandas_market_calendars as mcal\n",
    "from ftplib import FTP\n",
    "from pandas_datareader.data import DataReader as dr\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import zipfile\n",
    "import \n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "pd.set_option(\"display.max_columns\", 1000)re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "# define database name\n",
    "db = client.investment\n",
    "DB = client.HistoricalData\n",
    "collectionTreasury = 'TreasuryData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nyse calender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyse = mcal.get_calendar('NYSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "end_date   = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "early = nyse.schedule(start_date=start_date, end_date=end_date)\n",
    "business_days = mcal.date_range(early, frequency='1D')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ##get treasury data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## download daily treasury data\n",
    "def get_treasury(db):\n",
    "    syms = ['DGS6MO']\n",
    "    yc = dr(syms, 'fred') # could specify start date with start param here\n",
    "    names = dict(zip(syms, ['6m']))\n",
    "    yc = yc.rename(columns=names)\n",
    "    yc = yc[['6m']]\n",
    "\n",
    "    pp = pd.DataFrame(yc).reset_index()\n",
    "    data = pp.iloc[-1]\n",
    "    Interest_rate = data['6m']\n",
    "    dd = pd.DataFrame({'Interest_rate':[data['6m']],'Date':[data['DATE']]})\n",
    "    Data = dd.to_dict('records')\n",
    "    result = db.TreasuryData.insert_many(Data, ordered=True)\n",
    "    print('treasury data uploaded')\n",
    "    return (Interest_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### connnect with ftp server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make a connection to ftp server\n",
    "ftp = FTP('L3.deltaneutral.com')\n",
    "ftp.login(user='UserName', passwd = 'UserPassword')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grab zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get zip file list\n",
    "filenames = ftp.nlst()\n",
    "file = filenames[-1]\n",
    "\n",
    "# download zip file\n",
    "def grabFile(file):\n",
    "    filename = file\n",
    "    print(filename)\n",
    "    localfile = open(str(filename), 'wb')\n",
    "    ftp.retrbinary('RETR ' + filename, localfile.write, 1024)\n",
    "    ftp.quit()\n",
    "    localfile.close()\n",
    "    return (filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expiration labelling\n",
    "def expiration_group_def(data):\n",
    "    data['date_diff'] = (data['expiration'] - data['quote_date']).astype('timedelta64[D]')\n",
    "\n",
    "    # bins Creations for expiration_group\n",
    "    # print('Labeling expiration_group ... ')\n",
    "    bins_duration = [-float(\"inf\"), 7, 14, 30, 60, 90, 180, 365, 547, 730, float(\"inf\")] # Upto the number of days\n",
    "    labels_duration = ['0_week','1_week', '2_week', '1_month', '2_month', '3_month', '6_month', '1_year', '1.5_year', '2_year']\n",
    "    cat_duration = pd.cut(data['date_diff'], bins=bins_duration, labels=labels_duration)\n",
    "    Cat_duration = cat_duration.to_frame(name='expiration_group')\n",
    "    Duration_encoded = pd.concat([data, Cat_duration], axis=1)\n",
    "    return Duration_encoded\n",
    "\n",
    "def strike_group_def(data):\n",
    "    # Strike Encoding\n",
    "    data['Strike_diff'] = (1 - data['strike'] / data['underlying_bid_1545'])\n",
    "\n",
    "    # bins Creations for Strike_group\n",
    "    # print('Labeling Strike_group ...')\n",
    "    bins_strike =[-float(\"inf\"),-.70,-.60,-.50, -.40, -.30, -.20, -.10, 0, .10, .20, .30, .40, .50, .60, .70, 0.80, float(\"inf\")]\n",
    "    labels_strike = ['minus_other','minus_70','minus_60','minus_50', 'minus_40', 'minus_30', 'minus_20', 'minus_10', 'mid_point', 'plus_10', 'plus_20','plus_30', 'plus_40', 'plus_50', 'plus_60', 'plus_70','plus_other']\n",
    "    cat_strike = pd.cut(data['Strike_diff'], bins=bins_strike, labels=labels_strike)\n",
    "    Cat_strike = cat_strike.to_frame(name='Strike_group')\n",
    "    strike_encoded = pd.concat([data, Cat_strike], axis=1)\n",
    "    return strike_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## interest rate joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def interest_rate_joiner(data, db, collectionTreasury):\n",
    "    startdate = data['quote_date'].min()\n",
    "    enddate = data['quote_date'].max()\n",
    "    query = {\n",
    "        'Date': {\n",
    "            '$gte': startdate,\n",
    "            '$lte': enddate\n",
    "        }\n",
    "    }\n",
    "\n",
    "    project = {\n",
    "        '_id':0,\n",
    "        'Treasury': 1,\n",
    "        'Date': 1,\n",
    "    }\n",
    "    interest_rates = pd.DataFrame(list(db[collectionTreasury].find(query, project)))\n",
    "    if interest_rates.shape[0] != 0 :\n",
    "        interest_rates.dropna(subset=['Date'], inplace=True)\n",
    "        interest_rates = interest_rates.rename(columns={'Treasury': 'Interest_rate'})\n",
    "        interest_rates['Date'] = interest_rates['Date'].dt.tz_localize('UTC')   \n",
    "        interest_concat_data = data.merge(interest_rates, left_on='quote_date', right_on='Date', how='left')\n",
    "    else:\n",
    "        print('quote_date',data['quote_date'].iloc[0])\n",
    "        Interest_rate = get_treasury(db)\n",
    "        interest_rates = pd.DataFrame({'Interest_rate':[Interest_rate],'Date':[data['quote_date'].iloc[0]]})\n",
    "#         interest_rates['Date'] = interest_rates['Date'].dt.tz_localize('UTC')\n",
    "        print(interest_rates.head())\n",
    "        interest_concat_data = data.merge(interest_rates, left_on='quote_date', right_on='Date', how='left')\n",
    "    return interest_concat_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prerocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(data):\n",
    "    columns={'Gamma':'gamma_1545','Ask':'ask_1545','UnderlyingPrice':'underlying_bid_1545','Vega':'vega_1545',\n",
    "        'Theta':'theta_1545','OptionSymbol':'optionroot','Expiration':'expiration','UnderlyingSymbol':'symbol',\n",
    "        'Last':'last','Volume':'volume','Delta':'delta_1545','Strike':'strike','OpenInterest':'openinterest',\n",
    "        ' DataDate':'quote_date','IVMean':'implied_volatility_1545','Bid':'bid_1545','Type':'option'}\n",
    "    data.rename(columns=columns, inplace=True)\n",
    "    data.dropna(subset=['quote_date', 'expiration'], inplace=True)\n",
    "    data = data.drop(['Flags','T1OpenInterest','IVBid','IVAsk','AKA'], axis=1)\n",
    "    data['option'] = data['option'].map({'put': 'p', 'call': 'c'})\n",
    "    data = data.loc[data['option'] == 'p']\n",
    "    print(data.shape)\n",
    "    data['quote_date'] = pd.to_datetime(data['quote_date'], format=\"%m/%d/%Y\").dt.tz_localize('UTC')\n",
    "    data['expiration'] = pd.to_datetime(data['expiration'], format=\"%m/%d/%Y\").dt.tz_localize('UTC')\n",
    "    data = expiration_group_def(data)\n",
    "    data = strike_group_def(data)\n",
    "    data = interest_rate_joiner(data, db, collectionTreasury)\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload data to database useing chunk upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_options_data(db,DB,path):\n",
    "    print(\"Data uploading\")\n",
    "    skip = 0\n",
    "    select = 10000\n",
    "    rows_added = 0\n",
    "    max_rows = csv_length(path)\n",
    "    print(\" count : \"+str(max_rows))\n",
    "\n",
    "    if skip == 0:\n",
    "        data = pd.read_csv(path, nrows=select)       \n",
    "        data = process(data)\n",
    "        Data = data.to_dict('records')\n",
    "        try:\n",
    "            result = DB.OptionsDataTEMP.insert_many(Data, ordered=True)\n",
    "            skip = skip + select\n",
    "            rows_added = rows_added + len(data['quote_date'])\n",
    "            print(\"Added \"+str(rows_added)+\" rows.\")    \n",
    "            \n",
    "        except pymongo.errors.BulkWriteError as e:\n",
    "            print(e.details['writeErrors'])       \n",
    "        \n",
    "    while skip < max_rows:\n",
    "        print('Skip: '+str(skip)+', rows_added: '+str(rows_added))\n",
    "        data = pd.read_csv(path, nrows=select, skiprows=range(1,skip))        \n",
    "        data = process(data)\n",
    "        Data = data.to_dict('records')\n",
    "        try:\n",
    "            result = DB.OptionsDataTEMP.insert_many(Data, ordered=True)\n",
    "            skip = skip + select\n",
    "            rows_added = rows_added + len(data['quote_date'])\n",
    "            print(\"Added \"+str(rows_added)+\" rows.\")\n",
    "            \n",
    "        except pymongo.errors.BulkWriteError as e:\n",
    "            print(e.details['writeErrors'])      \n",
    "    print('option data upload complete')\n",
    "    return ()\n",
    "# L3_optionstats_20180807.csv\n",
    "\n",
    "def csv_length(path):\n",
    "    data = pd.read_csv(path)\n",
    "    return data.shape[0]\n",
    "\n",
    "def pricing_data(data):\n",
    "    data.rename(columns={'quotedate':'Date'}, inplace=True)\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format=\"%m/%d/%Y\").dt.tz_localize('UTC')\n",
    "    data.dropna(inplace=True)\n",
    "    return(data)\n",
    "\n",
    "def upload_price_data(db,DB,path):\n",
    "    print(\"Data uploading\")\n",
    "    skip = 0\n",
    "    select = 10000\n",
    "    rows_added = 0\n",
    "    max_rows = csv_length(path)\n",
    "    print(\" count : \"+str(max_rows))\n",
    "\n",
    "    if skip == 0:\n",
    "        data = pd.read_csv(path, nrows=select)\n",
    "        data = pricing_data(data)\n",
    "        Data = data.to_dict('records')\n",
    "        try:\n",
    "            DB.PricingDataTEMP.insert_many(Data, ordered=True)\n",
    "            skip = skip + select\n",
    "            rows_added = rows_added + len(data['Date'])\n",
    "            print(\"Added \"+str(rows_added)+\" rows.\")\n",
    "            \n",
    "        except pymongo.errors.BulkWriteError as e:\n",
    "            print(e.details['writeErrors'])  \n",
    "    \n",
    "    while skip < max_rows:\n",
    "        print('Skip: '+str(skip)+', rows_added: '+str(rows_added))\n",
    "        data = pd.read_csv(path, nrows=select, skiprows=range(1,skip))        \n",
    "        data = pricing_data(data)\n",
    "        Data = data.to_dict('records')        \n",
    "        try:\n",
    "            DB.PricingDataTEMP.insert_many(Data, ordered=True)\n",
    "            skip = skip + select\n",
    "            rows_added = rows_added + len(data['Date'])\n",
    "            print(\"Added \"+str(rows_added)+\" rows.\")\n",
    "        \n",
    "        except pymongo.errors.BulkWriteError as e:\n",
    "            print(e.details['writeErrors'])\n",
    "        print('pricing data upload complete')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedule auto job tofix time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job():\n",
    "    today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    if today in business_days:\n",
    "        # Interest_rate = get_treasury(db)\n",
    "        filename = grabFile(file)\n",
    "        print('filename:',filename)\n",
    "        # open zip file\n",
    "        fh = open(''+filename+'', 'rb')\n",
    "        z = zipfile.ZipFile(fh)\n",
    "        # unzip file and extract in folder\n",
    "        for name in z.namelist():\n",
    "            outpath = \"./daily_file\"            \n",
    "            z.extract(name, outpath)\n",
    "            if re.match(\"(.*)L3_options_(.*)\", outpath+'/'+name):\n",
    "                upload_options_data(db,DB,path=outpath+'/'+name)\n",
    "                os.remove(outpath+'/'+name)\n",
    "                print('options data uploaded')\n",
    "            elif re.match(\"(.*)L3_stockquotes_(.*)\",outpath+name):\n",
    "                upload_price_data(db,DB,path=outpath+'/'+name)\n",
    "                print('pricing data uploaded')\n",
    "                os.remove(outpath+'/'+name)\n",
    "        fh.close()\n",
    "    else:\n",
    "        print('today is not business day')\n",
    "\n",
    "schedule.every().day.at(\"17:28\").do(job)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################################\n",
    "######################################################\n",
    "\"\"\"\n",
    "class ftp_daily_optionsdata:\n",
    "    def __init__(self,db = db,DB = DB,collectionTreasury = collectionTreasury):\n",
    "        self.db = db\n",
    "        self.DB = DB\n",
    "        self.collectionTreasury = collectionTreasury\n",
    "\n",
    "    ## download daily treasury data\n",
    "    def get_treasury(self,db):\n",
    "        syms = ['DGS6MO']\n",
    "        yc = dr(syms, 'fred') # could specify start date with start param here\n",
    "        names = dict(zip(syms, ['6m']))\n",
    "        yc = yc.rename(columns=names)\n",
    "        yc = yc[['6m']]\n",
    "\n",
    "        pp = pd.DataFrame(yc).reset_index()\n",
    "        data = pp.iloc[-1]\n",
    "        Interest_rate = data['6m']\n",
    "        dd = pd.DataFrame({'Interest_rate':[data['6m']],'Date':[data['DATE']]})\n",
    "        Data = dd.to_dict('records')\n",
    "        result = db.TreasuryData.insert_many(Data, ordered=True)\n",
    "        print('treasury data uploaded')\n",
    "        return (Interest_rate)\n",
    "\n",
    "    # make a connection to ftp server\n",
    "    ftp = FTP('L3.deltaneutral.com')\n",
    "    ftp.login(user='to.green.michael', passwd = 'Pazzword24$')\n",
    "\n",
    "    # get zip file list\n",
    "    filenames = ftp.nlst()\n",
    "    file = filenames[-2]\n",
    "\n",
    "    # download zip file\n",
    "    def grabFile(self , file):\n",
    "        filename = file\n",
    "        print(filename)\n",
    "\n",
    "        localfile = open(str(filename), 'wb')\n",
    "        ftp.retrbinary('RETR ' + filename, localfile.write, 1024)\n",
    "\n",
    "        ftp.quit()\n",
    "        localfile.close()\n",
    "        return \n",
    "\n",
    "    # Expiration labelling\n",
    "    def expiration_group_def(self,data):\n",
    "        data['date_diff'] = (data['expiration'] - data['quote_date']).astype('timedelta64[D]')\n",
    "\n",
    "        # bins Creations for expiration_group\n",
    "        # print('Labeling expiration_group ... ')\n",
    "        bins_duration = [-float(\"inf\"), 7, 14, 30, 60, 90, 180, 365, 547, 730, float(\"inf\")] # Upto the number of days\n",
    "        labels_duration = ['0_week','1_week', '2_week', '1_month', '2_month', '3_month', '6_month', '1_year', '1.5_year', '2_year']\n",
    "        cat_duration = pd.cut(data['date_diff'], bins=bins_duration, labels=labels_duration)\n",
    "        Cat_duration = cat_duration.to_frame(name='expiration_group')\n",
    "        Duration_encoded = pd.concat([data, Cat_duration], axis=1)\n",
    "        return Duration_encoded\n",
    "\n",
    "    def strike_group_def(self , data):\n",
    "        # Strike Encoding\n",
    "        data['Strike_diff'] = (1 - data['strike'] / data['underlying_bid_1545'])\n",
    "\n",
    "        # bins Creations for Strike_group\n",
    "        # print('Labeling Strike_group ...')\n",
    "        bins_strike =[-float(\"inf\"),-.70,-.60,-.50, -.40, -.30, -.20, -.10, 0, .10, .20, .30, .40, .50, .60, .70, 0.80, float(\"inf\")]\n",
    "        labels_strike = ['minus_other','minus_70','minus_60','minus_50', 'minus_40', 'minus_30', 'minus_20', 'minus_10', 'mid_point', 'plus_10', 'plus_20','plus_30', 'plus_40', 'plus_50', 'plus_60', 'plus_70','plus_other']\n",
    "        cat_strike = pd.cut(data['Strike_diff'], bins=bins_strike, labels=labels_strike)\n",
    "        Cat_strike = cat_strike.to_frame(name='Strike_group')\n",
    "        strike_encoded = pd.concat([data, Cat_strike], axis=1)\n",
    "        return strike_encoded\n",
    "\n",
    "    def interest_rate_joiner(self , data, db, collectionTreasury):\n",
    "        startdate = data['quote_date'].min()\n",
    "        enddate = data['quote_date'].max()\n",
    "        query = {\n",
    "            'Date': {\n",
    "                '$gte': startdate,\n",
    "                '$lte': enddate\n",
    "            }\n",
    "        }\n",
    "\n",
    "        project = {\n",
    "            '_id':0,\n",
    "            'Treasury': 1,\n",
    "            'Date': 1,\n",
    "        }\n",
    "        interest_rates = pd.DataFrame(list(db[collectionTreasury].find(query, project)))\n",
    "        if interest_rates.shape[0] != 0 :\n",
    "            interest_rates.dropna(subset=['Date'], inplace=True)\n",
    "            interest_rates = interest_rates.rename(columns={'Treasury': 'Interest_rate'})\n",
    "            interest_rates['Date'] = interest_rates['Date'].dt.tz_localize('UTC')   \n",
    "            print(interest_rates.head())\n",
    "            interest_concat_data = data.merge(interest_rates, left_on='quote_date', right_on='Date', how='left')\n",
    "        else:\n",
    "            print('quote_date',data['quote_date'].iloc[0])\n",
    "            option.Interest_rate = get_treasury(db)\n",
    "            interest_rates = pd.DataFrame({'Interest_rate':[Interest_rate],'Date':[data['quote_date'].iloc[0]]})\n",
    "    #         interest_rates['Date'] = interest_rates['Date'].dt.tz_localize('UTC')\n",
    "            print(interest_rates.head())\n",
    "            interest_concat_data = data.merge(interest_rates, left_on='quote_date', right_on='Date', how='left')\n",
    "        return interest_concat_data\n",
    "\n",
    "    def process(self , data):\n",
    "        columns={'Gamma':'gamma_1545','Ask':'ask_1545','UnderlyingPrice':'underlying_bid_1545','Vega':'vega_1545',\n",
    "            'Theta':'theta_1545','OptionSymbol':'optionroot','Expiration':'expiration','UnderlyingSymbol':'symbol',\n",
    "            'Last':'last','Volume':'volume','Delta':'delta_1545','Strike':'strike','OpenInterest':'openinterest',\n",
    "            ' DataDate':'quote_date','IVMean':'implied_volatility_1545','Bid':'bid_1545','Type':'option'}\n",
    "        data.rename(columns=columns, inplace=True)\n",
    "        data.dropna(subset=['quote_date', 'expiration'], inplace=True)\n",
    "        data = data.drop(['Flags','T1OpenInterest','IVBid','IVAsk','AKA'], axis=1)\n",
    "        data['option'] = data['option'].map({'put': 'p', 'call': 'c'})\n",
    "        data = data.loc[data['option'] == 'p']\n",
    "        print(data.shape)\n",
    "        data['quote_date'] = pd.to_datetime(data['quote_date'], format=\"%m/%d/%Y\").dt.tz_localize('UTC')\n",
    "        data['expiration'] = pd.to_datetime(data['expiration'], format=\"%m/%d/%Y\").dt.tz_localize('UTC')\n",
    "        print(data.head())\n",
    "        data = expiration_group_def(data)\n",
    "        data = strike_group_def(data)\n",
    "        data = interest_rate_joiner(data, db, collectionTreasury)\n",
    "        \n",
    "        return(data)\n",
    "\n",
    "    def upload_options_data(self,db,DB,path):\n",
    "        print(\"Data uploading\")\n",
    "        skip = 0\n",
    "        select = 10000\n",
    "        rows_added = 0\n",
    "        max_rows = csv_length(path)\n",
    "        print(\" count : \"+str(max_rows))\n",
    "\n",
    "        if skip == 0:\n",
    "            data = pd.read_csv(path, nrows=select)       \n",
    "            data = process(data)\n",
    "            print(data.head())\n",
    "            Data = data.to_dict('records')\n",
    "            try:\n",
    "                result = DB.OptionsDataTEMP.insert_many(Data, ordered=True)\n",
    "                skip = skip + select\n",
    "                rows_added = rows_added + len(data['quote_date'])\n",
    "                print(\"Added \"+str(rows_added)+\" rows.\")    \n",
    "                \n",
    "            except pymongo.errors.BulkWriteError as e:\n",
    "                print(e.details['writeErrors'])       \n",
    "            \n",
    "        while skip < max_rows:\n",
    "            print('Skip: '+str(skip)+', rows_added: '+str(rows_added))\n",
    "            data = pd.read_csv(path, nrows=select, skiprows=range(1,skip))\n",
    "            \n",
    "            data = process(data)\n",
    "            Data = data.to_dict('records')\n",
    "            try:\n",
    "                result = DB.OptionsDataTEMP.insert_many(Data, ordered=True)\n",
    "                skip = skip + select\n",
    "                rows_added = rows_added + len(data['quote_date'])\n",
    "                print(\"Added \"+str(rows_added)+\" rows.\")\n",
    "                \n",
    "            except pymongo.errors.BulkWriteError as e:\n",
    "                print(e.details['writeErrors'])      \n",
    "        print('option data upload complete')\n",
    "        return ()\n",
    "    # L3_optionstats_20180807.csv\n",
    "\n",
    "    def csv_length(self,path):\n",
    "        data = pd.read_csv(path)\n",
    "        return data.shape[0]\n",
    "\n",
    "    def pricing_data(self,data):\n",
    "        data.rename(columns={'quotedate':'Date'}, inplace=True)\n",
    "        data['Date'] = pd.to_datetime(data['Date'], format=\"%m/%d/%Y\").dt.tz_localize('UTC')\n",
    "        data.dropna(inplace=True)\n",
    "        return(data)\n",
    "\n",
    "    def upload_price_data(self,db,DB,path):\n",
    "        print(\"Data uploading\")\n",
    "        skip = 0\n",
    "        select = 10000\n",
    "        rows_added = 0\n",
    "        max_rows = csv_length(path)\n",
    "        print(\" count : \"+str(max_rows))\n",
    "\n",
    "        if skip == 0:\n",
    "            data = pd.read_csv(path, nrows=select)\n",
    "            data = pricing_data(data)\n",
    "            Data = data.to_dict('records')\n",
    "            try:\n",
    "                DB.PricingDataTEMP.insert_many(Data, ordered=True)\n",
    "                skip = skip + select\n",
    "                rows_added = rows_added + len(data['Date'])\n",
    "                print(\"Added \"+str(rows_added)+\" rows.\")\n",
    "                \n",
    "            except pymongo.errors.BulkWriteError as e:\n",
    "                print(e.details['writeErrors'])  \n",
    "        \n",
    "        while skip < max_rows:\n",
    "            print('Skip: '+str(skip)+', rows_added: '+str(rows_added))\n",
    "            data = pd.read_csv(path, nrows=select, skiprows=range(1,skip))\n",
    "            \n",
    "            data = pricing_data(data)\n",
    "            Data = data.to_dict('records')        \n",
    "            try:\n",
    "                DB.PricingDataTEMP.insert_many(Data, ordered=True)\n",
    "                skip = skip + select\n",
    "                rows_added = rows_added + len(data['Date'])\n",
    "                print(\"Added \"+str(rows_added)+\" rows.\")\n",
    "            \n",
    "            except pymongo.errors.BulkWriteError as e:\n",
    "                print(e.details['writeErrors'])\n",
    "            print('pricing data upload complete')\n",
    "        return\n",
    "\n",
    "option = ftp_daily_optionsdata()\n",
    "\n",
    "def job(file):\n",
    "    today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    if today in business_days:\n",
    "        # Interest_rate = get_treasury(db)\n",
    "        zip_file = option.grabFile(file)\n",
    "        # open zip file\n",
    "        fh = open(''+file+'', 'rb')\n",
    "        z = zipfile.ZipFile(fh)\n",
    "        print(z)\n",
    "        # unzip file and extract in folder\n",
    "        for name in z.namelist():\n",
    "            print(name)\n",
    "            outpath = \"./daily_file\"            \n",
    "            z.extract(name, outpath)\n",
    "            print(outpath)\n",
    "            if re.match(\"(.*)L3_options_(.*)\", outpath+'/'+name):\n",
    "                print(outpath+'/'+name)\n",
    "                option.upload_options_data(db,DB,path=outpath+'/'+name)\n",
    "                os.remove(outpath+'/'+name)\n",
    "            elif re.match(\"(.*)L3_stockquotes_(.*)\",outpath+name):\n",
    "                option.upload_price_data(db,DB,path=outpath+'/'+name)\n",
    "                os.remove(outpath+'/'+name)\n",
    "        fh.close()\n",
    "    else:\n",
    "        print('today is not business day')\n",
    "\n",
    "schedule.every().day.at(\"15:13\").do(job(self,file))\n",
    "\n",
    "while True:\n",
    "schedule.run_pending()\n",
    "time.sleep(1)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
